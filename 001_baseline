{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":116062,"databundleVersionId":14875579,"sourceType":"competition"}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## 目的\nアミノ酸配列からGOを予測するモデルを作成する。\nGene Ontology (GO)：遺伝子や遺伝子製品の生物学的効果を表す階層モデルのこと。Molecular Function (MF), Biological Process (BP), Cellular Component (CC)の3つの親に分かれている。\n親から子にどんどん分岐していって、最終的にタンパク質の特定の機能という単一の機能にたどり着く。\n親も含め、すべての機能にはGOの番号がつけられているため、GO番号がわかれば、そのタンパク質の機能もわかる。","metadata":{}},{"cell_type":"code","source":"!pip install biopython > dev null","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T12:36:37.196482Z","iopub.execute_input":"2025-12-25T12:36:37.196830Z","iopub.status.idle":"2025-12-25T12:36:46.522544Z","shell.execute_reply.started":"2025-12-25T12:36:37.196804Z","shell.execute_reply":"2025-12-25T12:36:46.521082Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# =========================\n# Cell 1 — Imports & Config\n# =========================\nimport os\nimport gc\nimport random\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom torchmetrics.classification import MultilabelF1Score\nfrom sklearn.preprocessing import MultiLabelBinarizer, StandardScaler\nfrom collections import Counter, defaultdict\nfrom tqdm.auto import tqdm\nfrom Bio import SeqIO\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\n\nclass CFG:\n    # Path\n    TRAIN_TERMS = '/kaggle/input/cafa-6-protein-function-prediction/Train/train_terms.tsv'\n    TRAIN_SEQS = '/kaggle/input/cafa-6-protein-function-prediction/Train/train_sequences.fasta'\n    TRAIN_TAX = '/kaggle/input/cafa-6-protein-function-prediction/Train/train_taxonomy.tsv'\n    \n    TEST_SEQS = '/kaggle/input/cafa-6-protein-function-prediction/Test/testsuperset.fasta'\n    TEST_TAX = '/kaggle/input/cafa-6-protein-function-prediction/Test/testsuperset-taxon-list.tsv'\n    \n    OBO_FILE = '/kaggle/input/cafa-6-protein-function-prediction/Train/go-basic.obo'\n    \n    EMBEDDINGS_PATH = '/kaggle/input/cafa6-protein-embeddings-esm2/protein_embeddings.npy'\n    EMBEDDINGS_IDS = '/kaggle/input/cafa6-protein-embeddings-esm2/protein_ids.csv'\n    GOA_PATH = '/kaggle/input/goa-uniprot-all/selection_ids_goa_uniprot_all.csv' \n\n    # Global\n    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    THRESHOLDS = {'C': 0.08, 'F': 0.08, 'P': 0.01}\n    MIN_PREDS = 25\n\nensemble_configs = [\n    # 1. Aggressive\n    {'BS': 32, 'EPOCHS': 40, 'LR': 1e-3, 'D1': 1024, 'D2': 512, 'Drop': 0.3},\n    # 2. Wide & Robust\n    {'BS': 32, 'EPOCHS': 40, 'LR': 1e-3, 'D1': 2048, 'D2': 1024, 'Drop': 0.4},\n    # 3.Balanced Middle\n    {'BS': 32, 'EPOCHS': 40, 'LR': 1e-3, 'D1': 1024, 'D2': 768, 'Drop': 0.3}\n]\n\nprint(f\"Using device: {CFG.DEVICE}\")\nprint(f\"\\nEnsembling {len(ensemble_configs)} models for each aspect\")\n\n\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\n\nDATA_DIR = \"/kaggle/input/cafa-6-protein-function-prediction\"\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nCONFIG = {\n    \"batch_size\": 32,\n    \"max_len\": 512,   # maximum sequence length\n    \"embed_dim\": 128,\n    \"num_classes\": 50,  # example number of GO terms\n    \"device\": DEVICE\n}\nprint(f\"Using device: {CFG.DEVICE}\")\nprint(f\"\\nEnsembling {len(ensemble_configs)} models for each aspect\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T12:36:49.325379Z","iopub.execute_input":"2025-12-25T12:36:49.325756Z","iopub.status.idle":"2025-12-25T12:36:49.437867Z","shell.execute_reply.started":"2025-12-25T12:36:49.325722Z","shell.execute_reply":"2025-12-25T12:36:49.437027Z"}},"outputs":[{"name":"stdout","text":"Using device: cpu\n\nEnsembling 3 models for each aspect\nUsing device: cpu\n\nEnsembling 3 models for each aspect\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# 2. HELPER FUNCTIONS\ndef plot_history(history, aspect_name, model_name):\n    epochs = range(1, len(history['train_loss']) + 1)\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n    ax1.plot(epochs, history['train_loss'], 'r-o', label='Train Loss')\n    ax1.plot(epochs, history['val_loss'], 'b-s', label='Val Loss')\n    ax1.set_title(f'{aspect_name} [{model_name}] - Loss')\n    ax1.legend(); ax1.grid(True)\n    ax2.plot(epochs, history['val_f1'], 'g-^', label='Val F1')\n    ax2.set_title(f'{aspect_name} [{model_name}] - F1 Score')\n    ax2.legend(); ax2.grid(True)\n    plt.tight_layout(); plt.show()\n    \ndef parse_obo(obo_file):\n    print(f\"\\nParsing GO hierarchy from {os.path.basename(obo_file)}...\")\n    go_parents = defaultdict(list)\n    current_term = None\n    with open(obo_file, 'r') as f:\n        for line in f:\n            line = line.strip()\n            if line.startwith(\"id: GO:\"): current_term = line.split(\"id: \")[1]\n            elif line.startwith(\"is_a:\") and current_term:\n                go_parents[current_term].append(line.split(\"is_a: \")[1].split(\" !\")[0].strip())\n            elif line.startwith(\"relationship: part_of\") and current_term:\n                go_parents[current_term].append(line.split(\"part_of \")[1].split(\" !\")[0].strip())\n    print(f\"Loaded {len(go_parents):,} GO terms with parents\")\n    return go_parents\n\ndef parse_obo_children(obo_file):\n    go_children = {}\n    current_term = None\n\n    with open(obo_file, 'r') as f:\n        for line in f:\n            line = line.strip()\n            if line == '[Term]':\n                current_term = None\n            elif line.startswith('id: '):\n                current_term = line[4:]\n            elif line.startswith('is_a: ') and current_term:\n                parent = line[5:].split(' ! ')[0]\n                if parent not in go_children:\n                    go_children[parent] = []\n                go_children[parent].append(current_term)\n    return go_children\n\ndef get_all_ancestors(term, go_parents, cache):\n    if term in cache: return cache[term]\n    ancestors = set()\n    for parent in go_parents.get(term, []):\n        ancestors.add(parent)\n        ancestors.update(get_all_ancestors(parent, go_parents, cache))\n        cache[term] = ancestors\n        return ancestors\n\ndef get_descendants(term, children_map, cache=None):\n    if cache is not None and term in cache:\n        return cache[term]\n    descendants = set()\n    stack = [term]\n    # Simple DFS to find all children\n    while stack:\n        current = stack.pop()\n        if current in children_map:\n            kids = children_map[current]\n            # Only add kids we haven't seen to avoid cycles/dups\n            new_kids = [k for k in kids if k not in descendants]\n            descendants.update(new_kids)\n            stack.extend(new_kids)\n    if cache is not None:\n        cache[term] = descendants\n    return descendants\n\ndef propagate_predictions(df, go_parents):\n    print(\"\\nPropagating predictions to ancestors...\")\n    ancestor_cache = {}\n    propagated_rows = []\n    # Fast propagation\n    for pid, group in tqdm(df.groupby('pid'), desc=\"Propagationg\"):\n        term_scores = {}\n        for _, row in group.iterrows(): term_scores[row['term']] = max(term_scores.get(row['term'], 0), row['p'])\n        initial_terms = list(term_scores.keys())\n        for term in initial_terms:\n            score = term_scores[term]\n            for ancestor in get_all_ancestors(term, go_parents, ancestor_cache):\n                term_scores[ancestor] = max(term_scores.get(ancestor, 0), score)\n        for term, score in term_scores.items(): propagated_rows.append({'pid': pid, 'term': term, 'p': score})\n    return pd.DataFrame(propagated_rows)\n\nclass SequenceFeatureExtractor:\n    @staticmethod\n    def extract(seq):\n        if not seq: return np.zeros(85, dtype=np.float32)\n        try:\n            length = len(seq); aa_counts = Counter(seq)\n            standard = 'ACDEFGHIKLMPQRSTVWY' # 20 amino acids\n            aa_freq = np.array([aa_counts.get(aa, 0)/length for aa in standard], dtype=np.float32)\n            hydrophobic = sum(aa_counts.get(aa, 0) for aa in 'AILMFWYV') / length\n            charged = sum(aa_counts.get(aa, 0) for aa in 'DEKR') / length\n            aa_weights = {'A': 89, 'C': 121, 'D': 133, 'E': 147, 'F': 165, 'G': 75, 'H': 155, 'I': 131, 'K': 146, 'L': 131, 'M': 149, 'N': 132, 'P': 115, 'Q': 146, 'R': 174, 'S': 105, 'T': 119, 'V': 117, 'W': 204, 'Y': 181}\n            mol_weights = sum(aa_counts.get(aa, 0) * aa_weights.get(aa, 0) for aa in aa_counts)\n            props = np.array([np.log1p(length), hydrophobic, charged, np.log1p(mol_weight)], dtype=np.float32)\n            groups = {'hydro': 'AILMFWYV', 'polar': 'STNQ', 'pos': 'RK', 'neg': 'DE', 'arom': 'FWY', 'aliph': 'ILV'}\n            group_feats = np.array([sum(1 for aa in seq if aa in chars)/length for chars in groups.values()], dtype=np.float32)\n            top_di = ['AL', 'LA', 'LE', 'EA', 'AA', 'AS', 'SA', 'EL', 'LL', 'AE', 'SE', 'ES', 'GA', 'AG', 'VA', 'AV', 'LV', 'VL', 'LS', 'SL']\n            di_freq = np.zeros(20, dtype=np.float32)\n            if length > 1:\n                di_counts = Counter([seq[i:i+2] for i in range(length-1)])\n                di_freq = np.array([di_counts.get(dp, 0)/(length-1) for dp in top_di], dtype=np.float32)\n            top_tri = ['ALA', 'LEA', 'EAL', 'LAL', 'AAA', 'LLE', 'ELE', 'ALE', 'GAL', 'ASA', 'VLA', 'LAV', 'SLS', 'LSL', 'GLA', 'LAG', 'AVL', 'VLA', 'SLE', 'LES']\n            tri_freq = np.zeros(20, dtype=np.float32)\n            if length > 2:\n                tri_counts = Counter([seq[i:i+3] for i in range(length-2)])\n                tri_freq = np.array([tri_counts.get(tp, 0)/(length-2) for dp in top_tri], dtype=np.float32)\n            extra_di = ['RE', 'ER', 'VE', 'EV', 'TE', 'ET', 'AV', 'VA', 'GL', 'LG']\n            extra_freq = np.zeros(10, dtype=np.float32)\n            if length > 1:\n                di_counts = Counter([seq[i:i+2] for i in range(length-1)])\n                extra_freq = np.array([di_counts.get(dp, 0)/(length-1) for dp in extra_di], dtype=np.float32)\n            arom = sum(aa_counts.get(aa, 0) for aa in 'FWY') / length\n            instab = (hydrophobic + charged) / 2\n            iso = np.mean([ord(aa) for aa in seq[:100]])/255 if seq else 0\n            misc = np.array([arom, instab, iso, 0, 0], dtype=np.float32)\n            return np.concatenate([aa_freq, props, group_feats, di_freq, tri_freq, extra_freq, misc])\n        except: return np.zeros(85, dtype=np.float32)\n\n\ndef parse_fasta(path):\n    seqs = {}; feats = {}\n    print(f\"\\nParsing {os.path.basename(path)}...\")\n    for rec in tqdm(SeqIO.parse(path, \"fasta\"), disable=True):\n        pid = rec.id.split('|')[1] if '|' in rec.id else rec.id.split()[0]\n        s = str(rec.seq); seqs[pid] = s; feats[pid] = SequenceFeatureExtractor.extract(s)\n    return seqs, feats","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T12:21:00.803651Z","iopub.execute_input":"2025-12-30T12:21:00.803848Z","iopub.status.idle":"2025-12-30T12:21:00.832534Z","shell.execute_reply.started":"2025-12-30T12:21:00.803829Z","shell.execute_reply":"2025-12-30T12:21:00.831007Z"}},"outputs":[{"traceback":["\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_47/128635369.py\"\u001b[0;36m, line \u001b[0;32m82\u001b[0m\n\u001b[0;31m    initial_terms = list(term_scores.keys())\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"],"ename":"IndentationError","evalue":"unexpected indent (128635369.py, line 82)","output_type":"error"}],"execution_count":1},{"cell_type":"code","source":"#3. Data Loading\nprint(\"\\mLoading ESM2 Embeddings...\")\nemb_ids = pd.read_csv(CFG.EMBETTINGS_IDS)['protein_id'].tolist()\nemb_data = np.load(CFG.EMBETTINGS_PATH)\nembed_dict = {pid: emb_data[i] for i, pid in enumerate(emb_ids)}\ndel emb_data; gc.collect()\nprint(f\"Loaded {len(embed_dict):,} embeddings\")\n\ntrain_seqs, train_feats = parse_fasta(CFG.TRAIN_SEQS)\ntest_seqs, test_feats = parse_fasta(CFG.TEST_SEQS)\n\nprint(\"\\nLoading Taxonomy...\")\ntrain_tax_df = pd.read_csv(CFG.TRAIN_TAX, sep~'\\t', header=None, names=['pid', 'taxid'])\ntest_tax_df = pd.read_csv(CFG.TEST_TAX, sep='\\t', header=None, names=['pid', 'taxid'])\ntop_taxa = train_tax_df['taxid'].value_counts().head(20).index.tolist()\n\ndef get_tax_vactor(pid, tax_map):\n    vec = np.zeros(len(top_taxa), dtype=np.float32)\n    if pid in tax_map and tax_map[pid] in top_taxa:\n        vac[top_taxa.index(tax_map[pid])] = 1.0\n    return vec\n\ntrain_tax_map = train_tax_df.set_index('pid')['taxid'].to_dict()\ntest_tax_map = test_tax_df.set_index('pid')['taxid'].to_dict()\n\nprint(\"\\nScaling Features...\")\nall_pids_train = list(train_feats.keys()); all_;pids_test = list(test_feats.keys())\nX_manual_train = np.array([train_feats[p] for p in all_pids_train])\nX_manual_test = np.array([test_feats[p] for p in all_pids_test])\n\nscaler = StandardScaler()\nX_manual_train = scaler.fit_transform(X_manual_train)\nX_manual_test = scaler.fit_transform(X_manual_test)\n\nfor i, pid in enumerate(all_pids_train):\n    train_feats[pid] = np.concatenate([X_manual_train[i], get_tax_vector(pid, train_tax_map)])\nfor i, pid in enumerate(all_pids_test):\n    test_feats[pid] = np.concatenate([X_manual_test[i], get_tax_vector(pid, test_tax_map)])\nprint(\"\\nData prepared successfully\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#4. Model & Dataset\nclass MLP(nn.Module):\n    # Dynamic init\n    def __init__(self, input_dim, num_classes, h1, h2, drop):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(input_dim, h1),\n            nn.BatchNorm1d(h1),\n            nn.ReLU(),\n            nn.Dropout(drop),\n            nn.Linear(h1, h2),\n            nn.BatchNorm1d(h2),\n            nn.ReLU(),\n            nn.Dropout(drop),\n            nn.Linear(h2, num_classes)\n        )\n    def forward(self, x): return self.net(x)\n\n\nclass HybridDataset(Dataset):\n    def __init__(self, pids, labels, emb_dict, feat_dict):\n        self.pids = pids; self.labels = labels; self.emb = emb_dict; self.feat = feat_dict\n    def __len__(self): return len(self.pids)\n    def __getitem__(self, idx):\n        pid = self.pids[idx]\n        x = np.concatenate([self.emb[pid], self.feat[pid]]).astype(np.float32)\n        if self.labels is not None:\n            y = self.labels[idx].toarray().flatten() if hasattr(self.labels[idx], 'toarray') else self.labels[idx]\n            return torch.from_numpy(x), torch.tensor(y, dtype=torch.float32)\n        return torch.from_numpy(x)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#5. Training loop\ntrain_terms = pd.read_csv(CFG.TRAIN_TERMS, sep='\\t')\naspects = {'Biological Process': 'P', 'Molecular Function': 'F', 'Cellular Component': 'C'}\nsubmission_dfs = []\n\nsample_pid = list(train_feats.keys())[0]\ninput_dim = 1280 + len(train_feats[sample_pid])\n\nfor long_name, short_code in aspects.items():\n    print(f\"\\n{'='*40}\\nTraining Aspect: {long_name} ({short_code})\\n{'='*40}\")\n\n    aspect_df = train_terms[train_terms['aspect'] == short_code]\n    prot_terms = aspect_df.groupby('EntryID')['term'].apply(list).to_dict()\n    valid_pids = [p for p in prot_terms.keys() if p in embed_dict and p in train_feats]\n    if not valid_pids: continue\n\n    mlb = MultiLabelBinarizer(sparse_output=True)\n    y_train = mlb.fit_transform([prot_terms[p] for p in valid_pids])\n    num_classes = len(mlb.classes_)\n\n    ds = HybridDataset(valid_pids, y_train, embed_dict, train_feats)\n\n    # Store trained models for this aspect\n    models_list = []\n\n    for i, conf in enumerate(ensemble_configs):\n        print(f\"\\nModel {i+1}/3: BS={conf['BS']}, Dim={conf['D1']}, Drop={conf['Drop']}\")\n\n        train_len = int(0.9 * len(ds))\n        train_ds, val_ds = random_split(ds, [train_len, len(ds)-train_len])\n        train_loader = DataLoader(train_ds, batch_size=conf['BS'], shuffle=True, num_workers=4)\n        val_loader = DataLoader(val_ds, batch_size=conf['BS'], num_workers=4)\n\n        model = MLP(input_dim, num_classes, conf['D1'], conf['D'])\n        opt = optim.AdamW(model.parameters(), lr=conf['LR'], weight_devay=1e-4)\n        crit = nn.BCEWithLogitsLoss()\n        sched = optim.lr_scheduler.CosineAnnealingLR(opt, T_max=conf['EPOCHS'], eta_min=1e-6)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =========================\n# Cell 3 — Dataset & Preprocessing\n# =========================\nAA_VOCAB = \"ACDEFGHIKLMNPQRSTVWY\"  # 20 amino acids\naa_to_idx = {aa: i+1 for i, aa in enumerate(AA_VOCAB)}  # reserve 0 for padding\n\ndef encode_sequence(seq, max_len=CONFIG[\"max_len\"]):\n    arr = np.zeros(max_len, dtype=np.int64)\n    for i, aa in enumerate(seq[:max_len]):\n        arr[i] = aa_to_idx.get(aa, 0)\n    return arr\n\nclass ProteinDataset(Dataset):\n    def __init__(self, sequences, labels=None):\n        self.sequences = sequences\n        self.labels = labels\n        \n    def __len__(self):\n        return len(self.sequences)\n    \n    def __getitem__(self, idx):\n        seq = self.sequences[idx]\n        arr = encode_sequence(seq)\n        if self.labels is not None:\n            return torch.tensor(arr), torch.tensor(self.labels[idx], dtype=torch.float32)\n        else:\n            return torch.tensor(arr), torch.zeros(CONFIG[\"num_classes\"])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T13:06:07.201708Z","iopub.execute_input":"2025-12-18T13:06:07.202526Z","iopub.status.idle":"2025-12-18T13:06:07.211709Z","shell.execute_reply.started":"2025-12-18T13:06:07.202491Z","shell.execute_reply":"2025-12-18T13:06:07.210628Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# =========================\n# Cell 4 — Model Definition\n# =========================\nclass ProteinModel(nn.Module):\n    def __init__(self, vocab_size=len(AA_VOCAB)+1, embed_dim=CONFIG[\"embed_dim\"], num_classes=CONFIG[\"num_classes\"]):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n        self.lstm = nn.LSTM(embed_dim, 256, batch_first=True, bidirectional=True)\n        self.fc = nn.Linear(512, num_classes)\n        self.sigmoid = nn.Sigmoid()\n        \n    def forward(self, x):\n        x = self.embedding(x)\n        out, _ = self.lstm(x)\n        out = out.mean(dim=1)\n        out = self.fc(out)\n        return self.sigmoid(out)\n\nmodel = ProteinModel().to(CONFIG[\"device\"])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T13:06:11.761038Z","iopub.execute_input":"2025-12-18T13:06:11.761430Z","iopub.status.idle":"2025-12-18T13:06:11.822163Z","shell.execute_reply.started":"2025-12-18T13:06:11.761404Z","shell.execute_reply":"2025-12-18T13:06:11.820884Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# =========================\n# Cell 5 — Inference\n# =========================\n# Example: dummy sequences for inference\ndummy_sequences = [\"MTEYKLVVVGAGGVGKSALTIQLIQNHFVDEYDPT\", \"GAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAG\"]\ntest_ds = ProteinDataset(dummy_sequences)\ntest_loader = DataLoader(test_ds, batch_size=CONFIG[\"batch_size\"], shuffle=False)\n\nmodel.eval()\npreds = []\n\nwith torch.no_grad():\n    for seqs, _ in test_loader:\n        seqs = seqs.to(CONFIG[\"device\"])\n        out = model(seqs)\n        preds.append(out.cpu().numpy())\n\npreds = np.concatenate(preds)\nprint(\"Predictions shape:\", preds.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T13:06:15.667136Z","iopub.execute_input":"2025-12-18T13:06:15.667473Z","iopub.status.idle":"2025-12-18T13:06:15.864863Z","shell.execute_reply.started":"2025-12-18T13:06:15.667448Z","shell.execute_reply":"2025-12-18T13:06:15.863972Z"}},"outputs":[{"name":"stdout","text":"Predictions shape: (2, 50)\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# =========================\n# Cell 6 — Submission\n# =========================\nsample_sub_path = os.path.join(DATA_DIR, \"sample_submission.tsv\")\nsample_sub = pd.read_csv(sample_sub_path, sep=\"\\t\")\n\nsubmission = sample_sub.copy()\nsubmission[\"term\"] = np.random.choice([\"GO:0008150\",\"GO:0003674\",\"GO:0005575\"], size=len(sample_sub))  # dummy predictions\n\nsubmission.to_csv(\"submission.tsv\", sep=\"\\t\", index=False)\nprint(\"✅ submission.tsv saved\")\nsubmission.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T13:06:21.653636Z","iopub.execute_input":"2025-12-18T13:06:21.654476Z","iopub.status.idle":"2025-12-18T13:06:21.703231Z","shell.execute_reply.started":"2025-12-18T13:06:21.654440Z","shell.execute_reply":"2025-12-18T13:06:21.701642Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_47/3141809220.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# =========================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msample_sub_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sample_submission.tsv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0msample_sub\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_sub_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0msubmission\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_sub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 626\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1921\u001b[0m                     \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1922\u001b[0m                     \u001b[0mcol_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1923\u001b[0;31m                 \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1924\u001b[0m                     \u001b[0mnrows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1925\u001b[0m                 )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m                 \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_low_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m                 \u001b[0;31m# destructive to chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_concatenate_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n","\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n","\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n","\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n","\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n","\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 3 fields in line 3, saw 4\n"],"ename":"ParserError","evalue":"Error tokenizing data. C error: Expected 3 fields in line 3, saw 4\n","output_type":"error"}],"execution_count":6}]}